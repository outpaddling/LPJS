<?xml version="1.0" encoding="UTF-8"?>

<book xmlns="http://docbook.org/ns/docbook" version="5.0">
    <title>LPJS - Lightweight, Portable Job Scheduler</title>
    
    <chapter>
        <title>Introduction</title>
        
        <para>
        LPJS is a resource manager and job scheduler for running batch
        jobs on one or more computers.  It can be used on a single machine
        in order to maximize utilization of CPUs and memory without
        oversubscribing the system, or on multiple networked computers
        organized as an HPC (high performance computing) cluster or
        HTC (high throughput computing) grid.
        </para>
        
        <para>
        Unlike most similar tools, LPJS is designed to be small, simple,
        easy to install and configure, and easy to use.  It provides an
        intuitive user interface, including menu-based operation for
        most common tasks.
        </para>
        
        <para>
        This manual is aimed at the systems manager, covering
        installation and configuration of LPJS.  For a user's guide,
        including general information on HPC clusters
        and HTC grids, see the Research Computing User's Guide at
        <ulink url="https://acadix.biz/publications.php">https://acadix.biz/publications.php</ulink>.
        </para>
    </chapter>
    
    <chapter>
        <title>Installation</title>
        
        <para>
        Installation should be performed using a package manager, such as
        FreeBSD ports or pkgsrc.  We maintain a FreeBSD port for use on
        FreeBSD and Dragonfly BSD, and a pkgsrc package that should work on
        any other POSIX platform, including other BSDs, most Linux
        distributions, macOS, Solaris-based systems, etc.
        Other package managers may be supported by third parties.  If you
        would like to add LPJS to your favorite package manager, see
        the instructions for packagers in the README at
        <ulink url="https://github.com/outpaddling/LPJS/">https://github.com/outpaddling/LPJS/</ulink>.
        </para>
        
        <para>
        See <ulink url="http://netbsd.org/~bacon/">http://netbsd.org/~bacon/</ulink>
        for an introduction to installing pkgsrc.
        </para>
        
        <para>
        LPJS uses <command>munge</command>
        <ulink url="https://github.com/dun/munge">(https://github.com/dun/munge)</ulink>
        to encrypt and authenticate messages between nodes.  Munge is installed
        automatically by the package manager when installing LPJS, and
        automatically configured by LPJS administration scripts.
        </para>
        
        <para>
        Munge requires all nodes to have a shared munge key
        file, which is unique to your installation.  It must be generated
        by you and distributed to all
        computers that are part of your cluster or grid.
        THE MUNGE KEY FILE MUST BE KEPT SECURE AT ALL TIMES ON ALL NODES.
        Use secure procedures
        to distribute it to all nodes, so that it is never visible to
        unauthorized users.
        </para>
    </chapter>
    
    <chapter>
        <title>Configuration</title>
        
        <para>
        LPJS is designed to require minimal configuration.  For example,
        compute node resources such as processors and memory are determined
        automatically, and need not be specified in configuration files.
        </para>
        
        <para>
        Most configuration can be done entirely using
        <command>lpjs admin</command>
        (<command>man lpjs-admin</command>), a menu-driven admin tool.
        Simply run <command>lpjs-admin</command>, select an item from
        the menu, and answer the questions on the screen.
        </para>
        
        <para>
        For the sake of understanding what <command>lpjs-admin</command>
        does, some basic information is provided below.
        </para>
        
        <para>
        The head node requires a configuration file, which in its simplest
        form merely lists the complete host names (FQDNs) of the head node
        and each compute node, e.g.
        </para>
        
        <screen>
head    myhead.mydomain
compute compute001.mydomain
compute compute002.mydomain
...
        </screen>
        
        <para>
        The FQDN (fully qualified domain name) must match the name
        reported by the <command>hostname</command> command, or
        the <code>gethostname()</code> standard library function.
        This is usually either listed in <filename>/etc/hosts</filename>
        or provided by <glossterm>DNS</glossterm>
        (<glossterm>domain name service</glossterm>).
        </para>
        
        <para>
        Each compute node requires the same type of configuration file, but
        it need only list the head node.  It can be the same configuration
        file used on the head node, in which case the compute node entries
        are ignored.  You may wish to create a configuration file on the head
        node and simply distribute it.
        </para>
    </chapter>
    
    <chapter>
        <title>Starting Daemons</title>
        
        <section>
            <title>General Info</title>

            <para>
            All nodes in the cluster or grid must be running
            <command>munged</command>, using the
            same munge key.  The head node must also run
            <command>lpjs_dispatchd</command>, and
            all compute nodes must run <command>lpjs_compd</command>.
            </para>
            
            <para>
            Appropriate services can be configured by running
            <command>lpjs-admin</command> on each node.
            </para>
            
            <para>
            The head node can also serve as a compute node, though this is
            not generally recommended.  The head node of most clusters and
            grids should remain lightly
            loaded so that it can respond promptly to events that
            occur such as new job submissions and job completions.  The
            head node need not be a powerful machine.  A laptop or low-end
            desktop machine will work just fine for a small cluster.
            Laptops are actually nice in that they have a built-in battery
            backup, so your head node at least is protected against
            power outages.
            </para>
            
            <para>
            The head node on larger clusters need not be powerful, but
            should be highly reliable.  We recommend a server with a
            mirrored boot disk, hot-swap disks, and possibly redundant power
            supplies, also hot-swap.  With this hardware configuration,
            down time should be near zero.  FreeBSD makes it easy to
            mirror a boot disk using any computer with two drives.
            However, a hardware RAID card will make it easier to swap
            out a bad disk than a ZFS RAIDZ, assuming the disks are hot-swap.
            </para>
            
            <para>
            Jobs can be submitted from any node with the same version of
            LPJS and the shared munge key installed.
            </para>
            
            <note>
            It need not even be running LPJS daemons, but it
            does require a configuration file listing the head node, and munge
            to authenticate requests.
            </note>
            
            <para>
            Hence, other computers on the network
            can act as submit nodes, even if they are not part of the
            cluster/grid.
            </para>
        </section>
        
        <section>
            <title>Daemons as a Service</title>
            
            <para>
            The <command>lpjs_dispatchd</command> and
            <command>lpjs_compd</command> commands are normally run as a
            service, which automatically starts when the computer is
            rebooted.  You can run <command>lpjs admin</command> and use the
            menus
            to configure a machine as a head node or compute node with the
            appropriate services enabled.  This will require administrative
            rights on each computer in the cluster/grid.
            </para>
        </section>
        
        <section>
            <title>Ad hoc Clusters and Grids</title>

            <para>
            It is also possible to use LPJS without enabling services,
            even without having admin rights.
            Simply start the daemons manually by running
            <command>munged</command>
            and <command>lpjs_dispatchd</command> on the head node, and
            <command>munged</command> and <command>lpjs_compd</command>
            on each compute node.  Note that if <command>lpjs_compd</command>
            is not running as root,
            the compute node will only be able to run jobs
            under the same user name running <command>lpjs_compd</command>.
            </para>
            
            <para>
            The <command>lpjs ad-hoc</command> command displays a menu,
            allowing you to start and stop the appropriate daemons
            without having to know the precise commands.
            </para>
        </section>
    </chapter>
    
    <chapter>
        <title>Network topology</title>
        
        <para>
        A <glossterm>cluster</glossterm> is generally a collection of
        dedicated computers, all connected directly to the same private,
        often high speed network.  In this sense, a cluster is a LAN
        (local area network).  In many cases, special network technology,
        such as <glossterm>Infiniband</glossterm>, is used in place
        of, or in addition to, standard Ethernet.  Infiniband and
        similar technologies offer much lower latency per
        message, and higher throughput.
        </para>
        
        <para>
        A <glossterm>grid</glossterm> is conceptually like a cluster,
        in that it is used for distributed
        parallel computing.  However, grids are
        more loosely coupled, often utilizing computers that are not
        on the same LAN.  Hence, a grid is not as suitable for parallel
        computing that involves a lot of communication between processes,
        such as MPI (Message Passing Interface) distributed parallel programs.
        </para>
        
        <para>
        LPJS is very flexible with network topology.  The
        <emphasis>only</emphasis> requirement
        is that all nodes are able to connect to the head node.
        This means that a cluster or grid using LPJS can consist of
        other computers on the same LAN, computers in different buildings,
        virtual machines behind a <glossterm>NAT</glossterm>
        (<glossterm>network address translation</glossterm>) firewall,
        or cloud instances in a data center a thousand miles away.  You
        must consider how each of these resources can effectively be used.
        Network latency and throughput may be quite poor for resources
        that are far away.
        </para>
        
        <para>
        Clusters normally have some sort of file server, so that jobs
        run in a directory that is directly accessible from all nodes.
        This is the ideal situation, as input files are directly
        available to jobs, and output files from jobs can be written
        to their final location without needing to transfer them.
        </para>
        
        <para>
        Grids normally do not have file servers.  In this case, it will
        be necessary for all nodes to have the ability to pull files
        from and push files to <emphasis>somewhere</emphasis>.  Typically,
        this somewhere would be the submit node, or a server accessible
        for file transfers from the submit node and all compute nodes.
        </para>
        
        <note>
        All compute nodes must be able to perform a passwordless
        file transfers to the designated server.
        This is generally accomplished by installing ssh
        keys on the submit node, which can be done by running
        <command>auto-ssh-authorize submit-host</command> from every
        compute node, as every user who will run jobs.
        </note>
        
        <para>
        LPJS does not provide file
        transfer tools.  There are numerous highly-evolved, general-purpose
        file transfer tools already available, so it is left to the
        systems manager and user to decide which one(s) to use.
        We recommend using <command>rsync</command> if possible, as it
        is highly portable and reliable, and minimizes the amount of
        data transferred when repeating a transfer.
        </para>
        
        <para>
        If the working directory is not on a filesystem shared by the
        submit node and compute nodes, then the user's script is
        responsible for downloading any required input files.
        LPJS will, by default, transfer the entire temporary working
        directory back to the submit node, using
        <command>rsync -av temp-working-dir submit-host:working-dir</command>,
        where "working-dir" is the directory from which the job was submitted,
        and "temp-working-dir" is a job-specific temporary directory
        created by LPJS on the compute node.  Users can override this
        command.  See the Research Computing User Guide for details.
        </para>
        
        <para>
        The <command>lpjs submit</command> command creates a marker file
        in the working directory on the submit host, named
        "lpjs-submit-host-name-shared-fs-marker" (replace "submit-host-name"
        with the FQDN of your submit node).  If this file is not accessible
        to the compute node, then LPJS will take the necessary steps
        to create the temporary working directory and transfer it back
        to the submit node after the script terminates.
        </para>
    </chapter>
    
    <chapter>
        <title>Advanced configuration</title>
        
        <para>
        macOS nodes may require that full disk access be granted to
        lpjs_compd in System Settings, Privacy and Security.  Otherwise,
        you may see "operation not permitted" errors in the log when
        trying to access NFS shares and other directories that Apple
        has deemed worthy of privacy protections.
        </para>
    </chapter>
</book>
