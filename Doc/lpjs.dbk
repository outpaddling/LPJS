<?xml version="1.0" encoding="UTF-8"?>

<book xmlns="http://docbook.org/ns/docbook" version="5.0">
    <title>LPJS - Lightweight, Portable Job Scheduler</title>
    
    <chapter>
        <title>Introduction</title>
        
        <para>
        LPJS is a resource manager and job scheduler for running batch
        jobs on one or more computers.  It can be used on a single machine
        in order to maximize utilization of CPUs and memory without
        oversubscribing the system, or on multiple networked computers
        organized as an HPC (high performance computing) cluster or
        HTC (high throughput computing) grid.
        </para>
        
        <para>
        Unlike most similar tools, LPJS is designed to be small, simple,
        easy to install and configure, and easy to use.  It provides an
        intuitive user interface, including menu-based operation for
        most common tasks.
        </para>
        
        <para>
        This manual is aimed at the systems manager, covering
        installation and configuration of LPJS.  For a user's guide,
        including general information on HPC clusters
        and HTC grids, see the Research Computing User's Guide at
        <ulink url="https://acadix.biz/publications.php">https://acadix.biz/publications.php</ulink>.
        </para>
    </chapter>
    
    <chapter>
        <title>Installation</title>
        
        <para>
        Installation should be performed using a package manager, such as
        FreeBSD ports or pkgsrc.  We maintain a FreeBSD port for use on
        FreeBSD and Dragonfly BSD, and a pkgsrc package that should work on
        any other POSIX platform, including other BSDs, most Linux
        distributions, macOS, Solaris-based systems, etc.
        Other package managers may be supported by third parties.  If you
        would like to add LPJS to your favorite package manager, see
        the instructions for packagers in the README at
        <ulink url="https://github.com/outpaddling/LPJS/">https://github.com/outpaddling/LPJS/</ulink>.
        </para>
        
        <para>
        See <ulink url="http://netbsd.org/~bacon/">http://netbsd.org/~bacon/</ulink>
        for an introduction to installing pkgsrc.
        </para>
        
        <para>
        LPJS uses <command>munge</command>
        <ulink url="https://github.com/dun/munge">(https://github.com/dun/munge)</ulink>
        to encrypt and authenticate messages between nodes.  Munge is installed
        automatically by the package manager when installing LPJS, and
        automatically configured by LPJS administration scripts.
        </para>
        
        <para>
        Munge requires all nodes to have a shared munge key
        file, which is unique to your installation.  It must be generated
        by you and distributed to all
        computers that are part of your cluster or grid.
        THE MUNGE KEY FILE MUST BE KEPT SECURE AT ALL TIMES ON ALL NODES.
        Use secure procedures
        to distribute it to all nodes, so that it is never visible to
        unauthorized users.
        </para>
    </chapter>
    
    <chapter>
        <title>Configuration</title>
        
        <para>
        LPJS is designed to require minimal configuration.  For example,
        compute node resources such as processors and memory are determined
        automatically, and need not be specified in configuration files.
        </para>
        
        <para>
        Most configuration can be done entirely using
        <command>lpjs admin</command>
        (<command>man lpjs-admin</command>), a menu-driven admin tool.
        Simply run <command>lpjs-admin</command>, select an item from
        the menu, and answer the questions on the screen.
        </para>
        
        <para>
        For the sake of understanding what <command>lpjs-admin</command>
        does, some basic information is provided below.
        </para>
        
        <para>
        The head node requires a configuration file, which in its simplest
        form merely lists the complete host names (FQDNs) of the head node
        and each compute node, one node per line, e.g.
        </para>
        
        <screen>
head    myhead.mydomain
compute compute001.mydomain
compute compute002.mydomain
...
        </screen>
        
        <para>
        The FQDN (fully qualified domain name) must match the name
        reported by the <command>hostname</command> command, or
        the <code>gethostname()</code> standard library function.
        This is usually either listed in <filename>/etc/hosts</filename>
        or provided by <glossterm>DNS</glossterm>
        (<glossterm>domain name service</glossterm>).
        </para>
        
        <para>
        Hardware specs for compute nodes are automatically determined by
        lpjs_compd and reported to lpjs_dispatchd.  They may be overridden
        in the config file in order to reserve memory or processors for
        non-LPJS use, e.g.:
        </para>
        
        <screen>
head    myhead.mydomain
compute compute001.mydomain
# compute-002 actually have 16GiB RAM and 8 processors, but
# we limit LPJS to half of each
compute compute002.mydomain pmem=8GiB procs=4
...
        </screen>
        
        <para>
        Each compute node requires the same type of configuration file, but
        it need only list the head node.  It can be the same configuration
        file used on the head node, in which case the compute node entries
        are ignored.  You may wish to create a configuration file on the head
        node and simply distribute it.
        </para>
    </chapter>
    
    <chapter>
        <title>Starting Daemons</title>
        
        <section>
            <title>General Info</title>

            <para>
            All nodes in the cluster or grid must be running
            <command>munged</command>, using the
            same munge key.  The head node must also run
            <command>lpjs_dispatchd</command>, and
            all compute nodes must run <command>lpjs_compd</command>.
            </para>
            
            <para>
            Appropriate services can be configured by running
            <command>lpjs-admin</command> on each node.
            </para>
            
            <para>
            The head node can also serve as a compute node, though this is
            not generally recommended.  The head node of most clusters and
            grids should remain lightly
            loaded so that it can respond promptly to events that
            occur such as new job submissions and job completions.  The
            head node need not be a powerful machine.  A laptop or low-end
            desktop machine will work just fine for a small cluster.
            Laptops are actually nice in that they have a built-in battery
            backup, so your head node at least is protected against
            power outages.
            </para>
            
            <para>
            The head node on larger clusters need not be powerful, but
            should be highly reliable.  We recommend a server with a
            mirrored boot disk, hot-swap disks, and possibly redundant power
            supplies, also hot-swap.  With this hardware configuration,
            down time should be near zero.  FreeBSD makes it easy to
            mirror a boot disk using any computer with two drives.
            However, a hardware RAID card will make it easier to swap
            out a bad disk than a ZFS RAIDZ, assuming the disks are hot-swap.
            </para>
            
            <para>
            Jobs can be submitted from any node with the same version of
            LPJS and the shared munge key installed.
            </para>
            
            <note>
            It need not even be running LPJS daemons, but it
            does require a configuration file listing the head node, and munge
            to authenticate requests.
            </note>
            
            <para>
            Hence, other computers on the network
            can act as submit nodes, even if they are not part of the
            cluster/grid.
            </para>
        </section>
        
        <section>
            <title>Daemons as a Service</title>
            
            <para>
            The <command>lpjs_dispatchd</command> and
            <command>lpjs_compd</command> commands are normally run as a
            service, which automatically starts when the computer is
            rebooted.  You can run <command>lpjs admin</command> and use the
            menus
            to configure a machine as a head node or compute node with the
            appropriate services enabled.  This will require administrative
            rights on each computer in the cluster/grid.
            </para>
        </section>
        
        <section>
            <title>Ad hoc Clusters and Grids</title>

            <para>
            It is also possible to use LPJS without enabling services,
            even without having admin rights.
            Simply start the daemons manually by running
            <command>munged</command>
            and <command>lpjs_dispatchd</command> on the head node, and
            <command>munged</command> and <command>lpjs_compd</command>
            on each compute node.  Note that if <command>lpjs_compd</command>
            is not running as root,
            the compute node will only be able to run jobs
            under the same user name running <command>lpjs_compd</command>.
            </para>
            
            <para>
            The <command>lpjs ad-hoc</command> command displays a menu,
            allowing you to start and stop the appropriate daemons
            without having to know the precise commands.
            </para>
        </section>
    </chapter>
    
    <chapter>
        <title>Network topology</title>
        
        <para>
        A <glossterm>cluster</glossterm> is generally a collection of
        dedicated computers, all connected directly to the same private,
        often high speed network.  In this sense, a cluster is a LAN
        (local area network).  In many cases, special network technology,
        such as <glossterm>Infiniband</glossterm>, is used in place
        of, or in addition to, standard Ethernet.  Infiniband and
        similar technologies offer much lower latency per
        message, and higher throughput.
        </para>
        
        <para>
        A <glossterm>grid</glossterm> is conceptually like a cluster,
        in that it is used for distributed
        parallel computing.  However, grids are
        more loosely coupled, often utilizing computers that are not
        on the same LAN.  Hence, a grid is not as suitable for parallel
        computing that involves a lot of communication between processes,
        such as MPI (Message Passing Interface) distributed parallel programs.
        </para>
        
        <para>
        LPJS is very flexible with network topology.  The
        <emphasis>only</emphasis> requirement
        is that all nodes are able to connect to the head node.
        This means that a cluster or grid using LPJS can consist of
        other computers on the same LAN, computers in different buildings,
        virtual machines behind a <glossterm>NAT</glossterm>
        (<glossterm>network address translation</glossterm>) firewall,
        or cloud instances in a data center a thousand miles away.  You
        must consider how each of these resources can effectively be used.
        Network latency and throughput may be quite poor for resources
        that are far away.
        </para>
    </chapter>
    
    <xi:include xmlns:xi="http://www.w3.org/2001/XInclude"
        href="file-sharing.dbk" />
    
    <chapter>
        <title>Advanced configuration</title>
        
        <para>
        TBD
        </para>
    </chapter>
</book>
